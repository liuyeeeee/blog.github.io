[{"title":"multivariable calculus","url":"/2024/04/15/mutivariable-calculus/","content":"\n\n\n------\n\n<!--more-->\n\n# Different Vectors\n\n即使 position vector 和 direction vector 都是 vector，但它们表达的概念却有差异\n\n## position vector\n\n如字面意思是用以表达位置的向量。为方便计算，我们将 $R^n$ 空间中的  $n$ 个分量分离，以向量的形式表达一个 0 为起始位置的向量。例如，在scalar field 中我们使用 $(1,2)$，但在vector field 中我们使用 $1\\hat{i}+2\\hat{j}$ 或者简化为 $\\left<1,2\\right>$ 来表达。\n\n## direction vector\n\n这类向量是用作表示移动方向和距离的向量，其起始位置并不重要，我们可以自由地移动这类向量使其作用在任意 position vector 上，来形成新的向量。\n\n{%dplayer url='vector_type.mp4' \"autoplay=false\" %}\n\n## tagent vector\n\n线或面的切向量，垂直于法向量，代表着该点的斜率\n$$\nT(t)=\\frac{r'(t)}{|r'(t)|}\n$$\n$r'(t)$ refere to the [Cartesian Form](#cartesian_equation)\n\n## normal vector\n\n法向量垂直于平面的向量，\n\n我们可以通过 tagent vector 求 unit normal vector\n$$\nN(t)=\\frac{T'(t)}{|T'(t)|}\n$$\n\n\n## <span id=\"cartesian_equation\">Cartesian equation</span>\n\n> take $\\left<x,y,z\\right>=<a_1,a_2,a_3>+t\\left<d_1,d_2,d_3\\right>$ as example\n\n### <span id=\"1\">Parametric Form</span>\n\n$$\n\\left\\\\{\n\\begin{aligned}\nx&=a_1+td_1\\\\\\\\\ny&=a_2+td_2\\\\\\\\\nz&=a_3+td_3\n\\end{aligned}\n\\right.\\space,\\space t\\in\\mathbb R\n$$\n\n### Symmetric Form\n\n$$\n\\frac{x-a_1}{d_1}=\\frac{y-a_2}{d_2}=\\frac{z-a_3}{d_3}\n$$\n\n\n\n# Operations\n\n## <span id=\"3\">dot product (点积)</span>\n\n$$\n\\vec a\\cdot\\vec b=\\left|\\left|\\vec a\\right|\\right|\\left|\\left|\\vec b\\right|\\right|\\cos\\theta\n$$\n需要小心 $\\theta$ 是向量头部相交的夹角。\n\n## cross product (叉积)\n\n$$\n\\vec a\\times\\vec b=\\left|\\left|\\vec a\\right|\\right|\\left|\\left|\\vec b\\right|\\right|\\sin\\theta\n$$\n\n物理层面而言，叉积的目的在于寻找垂直于 $\\vec{a}$ 和 $\\vec b$ 的向量。\n\n# vector-valued function\n\n$$\n\\vec{r}(t)=\\left<f(t),g(t),h(t)\\right>\n$$\n\n我们通过把$x,y,z$ 的值以$t$ 为唯一 parameter 的equation 来表达，一方面简化了我们的计算吗，另一方面也让我们以等式表达了vector 在 $a\\leq t\\leq b$ 区间之中的变化。\n\n## Derivatives\n\n$$\n\\begin{aligned}\n\\frac{d\\vec r}{dt}=\\vec r^\\prime(t)=\\lim_{h\\to 0}\\frac{\\vec r(t+h)-\\vec r(t)}{h}\\\\\n\\vec r^\\prime(t)=f^\\prime(t)\\hat i+g^\\prime(t)\\hat j+h^\\prime(t)\\hat k\n\\end{aligned}\n$$\n\n### <span id=\"2\">tangent line</span>\n\n由于 $\\vec{r}(t)$ 是一个向量，且通常是一个 direction vector 所以当我们要找tagent line 时并不能直接其当为 $\\vec r(t)$，而是要注意到，tagent line 应该与 $\\vec{r}(t)$ 平行。 即参考 [Parametric Form](#1)，其中的 $d_1,d_2,d_3$，应与$f^\\prime(t),g^\\prime(t),h^\\prime(t)$ 保持一致以确保tagent line 在 $x,y,z$ 分量上的移动与tagent vector一致。\n\n#### Parametric Form\n\n$$\n\\left\\\\{\n\\begin{aligned}\nx&=a_1+tf^\\prime(t)\\\\\\\\\ny&=a_2+tg^\\prime(t)\\\\\\\\\nz&=a_3+th^\\prime(t)\n\\end{aligned}\n\\right.\\space,\\space t\\in\\mathbb R\n$$\n\n#### Symmetric Form\n\n$$\n\\frac{x-a_1}{f^\\prime(t)}=\\frac{y-a_2}{g^\\prime(t)}=\\frac{z-a_3}{h^\\prime(t)}\n$$\n\n## Arc length calculation\n\nAs $\\vec r(t)$ 是每一点的tagent，可以理解为非常短的距离内的移动，通过 寻找vector magnitude ( $\\left|\\left|\\vec r(t)\\right|\\right|$) 的方法，我们找到每一小段的长度，将其相加就是我们要的弧长\n$$\n\\text{Length}=\\int_a^b\\sqrt{\\left[f^\\prime(t)\\right]^2+\\left[g^\\prime(t)\\right]^2} \\space d t \\quad \\text{for}\\space a\\leq r\\leq b\n$$\n\n# Partial Derivatives\n\nBasic concept 是将，除目标变量以外的其他变量当成常数。以$\\frac{\\partial}{\\partial x}$ 为例，只将$x$ 当为变量。\n\nLet $z=f(x,y)$，当我们要微分$f(x,y)$ with respect to $x,y,z$ 时，会有不同的结果。为方便我们的理解，不妨这样想，\n$$\nf_x=\\frac{\\partial z}{\\partial x}\n$$\n这样算出来的微分是在 $z$ 和 $x$ 变量外的变量的（也就是$y$）平面上进行的。\n\n## Tagent equation by using Partial Derivative\n\n在[tangent line](#2)中,我们只讨论了如何从$\\vec{r}(t)$找到tagent line，在这部分我们将了解到如何直接从最基本的$z=f(x,y)$ 寻找tagent equation on the plane。\n\n> 斜面如何处理？\n\nLet $z=10-4x^2-y^2$，要找point $(1,2)$ 在plane $y=2$ 的tagent 时，我们要先找$\\frac{dz}{dx}|_{y=2}$ 的slope，即 $-8$。根据slope 的定义我们知道，$\\frac{dz}{dx}=-8$ 的意思是，$x$ 移动 1 unit $z$ 就会减少 8 units，加上我们是给予$y=2$的平面做微分。由此可得，tagent equation 的direction vector 是$\\left<1,0,-8\\right>$，我们也拥有$z=f(x,y)$ 的公式，可由 $f(1,2)$ 找到 $z$ 的位置。最后，我们会得到 $\\vec r(t)=\\left<1,2,2\\right>+t\\left<1,0,8\\right>$ 的 parametric form。\n\n# <span id=\"vector_equation_of_a_plane\">vector equation of a plane</span>\n\n设 $\\vec a=[a_1,a_2,a_3],\\vec r=[x,y,z]$ 在平面上($a_1,a_2,a_3$ 为已知数值)，$\\vec n$ 为平面的normal vector。根据[dot product](#3)的公式，当两个向量互相垂直时，点积为零，而无论$\\vec r=\\vec a$ 或者 $\\vec a-\\vec r$ 都完全贴在平面里，所以$(\\vec r-\\vec a)\\cdot\\vec n=0$, 即\n$$\n\\begin{aligned}(\\vec r-\\vec a)\\cdot\\vec n&=0=(\\vec a-\\vec r)\\cdot n\\\\\\\\\n\\vec r\\cdot\\vec n&=\\vec a\\cdot\\vec n=d\n\\end{aligned}\n$$\n现在，让我们直接把变量带进上面 :point_up_2:的公式里，\n$$\n\\begin{aligned}(\\vec r-\\vec a)\\cdot\\vec n&=0\\\\\\\\\n(\\left<x,y,z\\right>-\\left<a_1,a_2,a_3\\right>)\\cdot\\left<A,B,C\\right>&=0\\\\\\\\\nA(x-a_1)+B(y-a_2)+C(z-a_3)&=0\\\\\\\\\nAx+By+Cz&=D\n\\end{aligned}\n$$\n因为dot product 可以用来找projection，所以 $\\vec r\\cdot\\vec n=d=D$ 其实是平面离origin 的最小距离。我们可以得出normal vector $\\cdot \\left<x,y,z\\right>$ 就是平面离origin 的最短距离的结论。\n\n> cross product can be used to find normal vector\n\n# Gradient $\\nabla$\n\n梯度是一种向量，它总指向 function $f$ 上升最快的方向。记住，$\\nabla f(x,y)$永远垂直于$x,y$ 所在的那个contour line (等高线)。\n$$\n\\nabla f(x,y)=\\left<f_x(x,y),f_y(x,y)\\right>\n$$\n之所以垂直，我的理解是，上升最快的方向边是指向外层等高线的同时，离自己最近的点，而这个点与$(x,y)$的位置画一条线，就是我们的法线(normal line)。\n\n# Directional Derivative of a vector\n\n方向导数是 函数 $f$ 在特定 unit vector ($\\vec{u}=<a,b>$)下的 rate of chagne，公式为\n$$\n\\begin{aligned}\nD_{\\vec{u}}f(x,y)&=\\lim_{h\\to 0} \\frac{f(x+ha,y+hb)-f(x,y)}{h}\\\\\\\\\nD_{\\vec{u}}f(x,y)&=f_x(x,y)a+f_y(x,y)b\n\\end{aligned}\n$$\n要理解这个公式也十分简单，试将$f(x,y)$ 当成 $y$-axis 然后把 h 当作 $x$-axis ，然后通过计算 slope 就可以得到上的公式。从上面的公式中我们可以拆解，得到\n$$\nD_{\\vec{u}}f(x,y)=\\nabla f\\cdot\\left<a,b\\right>\n$$\n由于我们知道，$\\nabla f$ 是 $f$ 上升最快的方向，所以当$\\vec u$ 的方向与$\\nabla f$ 一致，那么Maximum rates of Change of $f$ 就是 $\\left|\\nabla f(x,y)\\right|$，因为 $\\nabla f\\cdot\\left<a,b\\right>=|\\nabla f||\\vec u|\\cos\\theta$。\n\n# find the tagent plane at a point\n\n这部分没有什么特别的只是对于 [vector equation of a plane](#vector_equation_of_a_plane) 的运用，由于 $\\nabla f$ 是normal vector 所以我们直接可以将 $\\nabla f$ 代进 $\\vec n$ 里\n$$\n\\nabla f(x_0,y_0,z_0)\\cdot\\left<x-x_0,y-y_0,z-z_0\\right>=0\n$$\n思考：在 $z=f(x,y)$ 的情况下如何找到 tagent plane 而不是tagent line [解答](#4)\n\n# Divergence\n\n$$\nDiv\\space\\vec F=\\nabla\\cdot \\vec F\n$$\n\n散度是一个数量，显示了向量场中某点的扩散和内缩的差值。\n\n- \\> 0 即发散\n- \\< 0 即内缩\n- =0 即既不发散也不内缩\n\n# Curl\n\n$$\n\\begin{aligned}\nCurl \\space\\vec F=\\nabla\\times \\vec F=\n\\left|\n\\begin{matrix}\n\\hat{i}&\\hat{j}&\\hat{k}\\\\\n\\frac{\\partial}{\\partial x}&\\frac{\\partial}{\\partial y}&\\frac{\\partial}{\\partial z}\\\\\nP &Q&R\n\\end{matrix}\n\\right|\n\\\\\\text{for}\\quad\\vec F=P\\hat i+Q\\hat j+R\\hat k\n\\end{aligned}\n$$\n\n# <span id=\"4\">find tangent plane by given a two variables function</span>\n\nThere are two method to use\n\n## Method 1\n\nLet a new function $F(x,y,z)=f(x,y)-z=0$\n\nThen the normal vector will be $\\left<f_x(x,y),f_y(x,y),-1\\right>$ \n\nBy using the vector equation of a plane, we get\n$$\n\\left<F(x_0,y_0,z_0)-F(x,y,z)\\right>\\cdot\\left<f_x(x,y),f_y(x,y),-1\\right>=0\n$$\n\n## Method 2 \n\n我们可以直接将$f(x,y)$ 当作 $z$ 分量。即$Q=F(x,y,f(x,y))$\n# 标量场（scalar field)\n\n<!--more-->\n\n##  处理新的变量 z\n\n在二维平面中，我们以 $y=f(x)$ 表示线条，而在三维空间中我们表示线条应该以 $z=f(x, y)$ 表示$x,y,z$ 之间的关系。\n\n## 计算面积\n\n二维空间中，我们计算以积分计算面积的方程为 \n$$\n\\text{Area}=\\int y\\ dx\n$$\n基于相同的概念，我们计算三维空间中的面积的公式为\n$$\n\\int_C z\\ ds\n$$\n其中 $C$ 为忽略z轴后，线的位置，s 则类似于二维空间中 $x$ 的角色，是 $C$ 上两点之间的弧长( distance but not displacement) 。由于 $ds$ 是一段极为短的距离，因此直接微分 $C$ 显然不是明智的选择。更合适的选择将 $s$ 拆分为 $x$ 函数和 $y$ 函数，以向量表达 $C$，\n$$\n\\vec{r}(t)=[ x(t),y(t)]\n$$\n然后通过找这个 vector  的 magnitude 来代替 $ds$ \n$$\n\\frac{ds}{dt}=\\left|\\left|r^\\prime(t)\\right|\\right|=\\sqrt{\\left(\\frac{dx}{dt}\\right)^2+\\left(\\frac{dy}{dt}\\right)^2}\n$$\n请注意，在积分中并不能存在多余一个变量，因此函数 $f$ 也需要以 $t$ 来表示，由于 $\\vec{r}(t)$ 中已经包含了 $f$ 需要经过的 $x,y$ ，因此我们可以将 $f(x,y)$ 直接写做 $f(x(t),y(t))$ 。\n\n## 完整方程\n\n$$\n\\text{Area}=\\int_a^bf(x(t),y(t))\\left|\\vec{r}^\\prime(t)\\right|dt\n$$\n\n> $a$ and $b$ are the range of $C$\n\n\n\n# 向量场\n\n在向量场中，计算并没有*标量场* 中来得直接，不过差别也不大。基本概念依然是\n$$\n\\int_C\\vec{F}\\cdot d\\vec{r}\n$$\n这里我们直接使用 $\\vec{r}$ 替代 $s$ ，因为向量场中不存在标量 $s$ 。如同在标量场中一样，我们依然需要以一个变量表达，\n$$\n\\int_C\\vec{F}(\\vec{r}(t))\\cdot d\\vec{r}=\\int_a^b\\vec{F}(\\vec{r}(t))\\cdot\\vec{r}^\\prime(t)dt\n$$\n\n# Conservative vector field\n\n在向量场中，如果 多个向量的起点与终点一致，则他们的line integral 永远一样，换句话说在保守向量场中，向量的line integral 只受起点与终点影响。\n\n这个向量场的构成取决与在数量场中的 potential function。\n$$\n\\vec F=\\nabla f\n$$\nBy using the properties of Gradient ($\\frac{\\partial}{\\partial x},\\frac{\\partial}{\\partial y},\\frac{\\partial}{\\partial z}$)，我们可以算得\n$$\n\\begin{aligned}\\int_C\\nabla f\\cdot d\\vec r=\\int_C F\\cdot d\\vec r&=\\int_a^b\\nabla f(\\vec r(t))\\cdot\\vec r'(t)dt\\\\\\\\\\&=\\int_a^b\\frac{d}{dt}[f(\\vec r(t))]d\\\\\\\\\n&=f(\\vec r(b))-f(\\vec r(a))\n\\end{aligned}\n$$\n\n## find potential function $f$\n\n因为微分会帮我们剔除一些无关的变量，例如$\\frac{\\partial}{\\partial x}(x^2+y^2+z^2)=2x$，因此在做积分的时候我们不应该单纯以constant $C$ 来替代 function。\n$$\n\\begin{aligned}\n\\text{Assume} \\quad\\vec F(x,y,z)&=\\left<P(x,y,z),Q(x,y,z),R(x,y,z)\\right>\\\\\\\\\nf(x,y,z)&=\\int Pdx=\\text{function of }x+G(y,z)\\\\\\\\\nf(x,y,z)&=\\int Qdy=\\text{function of }y+H(x,z)\\\\\\\\\nf(x,y,z)&=\\int Rdz=\\text{function of }z+K(x,y)\\\\\\\\\n\\end{aligned}\n$$\n因为 $G(y,z),H(x,z),K(x,y)$ 的完整公式一定充 $f(x,y,z)$ 中将各自的部分抽离出来再加上一个隐藏的 常数。因此，\n$$\n\\begin{aligned}\nG(y,z)&=\\text{function of y + function of z}+ C_1\\\\\\\\\nH(x,z)&=\\text{function of x + function of z}+ C_2\\\\\\\\\nK(x,y)&=\\text{function of x + function of y}+ C_3\\\\\\\\\n\\end{aligned}\n$$\n由于在 $G,H,K$ 中有重复的参数，所以很容易想到会出现重复的项，最后我们可以通过剔除重复项得到\n$$\nf(x,y,z)=\\text{function of x + functio of y + function z }+ C\n$$\n\n## prove whether integral independent on the path\n\n由于我们知道，\n$$\n\\begin{aligned}\nf(x,y,z)&=\\int Pdx\\\\\\\\\nf(x,y,z)&=\\int Qdy\\\\\\\\\nf(x,y,z)&=\\int Rdz\\\\\\\\\n\\end{aligned}\n$$\n但是我们不应该直接计算积分的结果应为他总会包含一个未知函数，实际上我们做微分会更有效率，\n\n## if path independent, then for any closed path C\n$$\n\\oint_C F(r)\\cdot dr=0\n$$\n\n## Exactness of Differential Form ( or Pfaffian form )\n\n\n\n# Double Integral\n\n对于 single integral 来说，$\\int ydx$ 得到的是二维的面积。但是如果我们将二维的面积再一次积分，我们就可以的到一个三维的体积，公式为，\n$$\n\\text{Volume}={\\iint}_Rf(x,y)\\space dA\n$$\n\n> A 是我们的面积，$f(x,y)$ 可以看作高度。\n\n在 double integral 中，我们只有一点要注意，在 single integral 中，我们只需要扫过 $x$ 或 $y$ 就能计算到面积，但我们不能使用一个固定的数值来描述由多个 区间 bounded 起来的range。所以我们需要用equation 来解决这个问题。例如，\n$$\n\\int^1_0\\int^{\\sqrt x}_{x^3}f(x,y)\\space dydx\n$$\n\n> 外层积分需要是常数\n\n## Convert Cartesian Form into Parametric form (p.15)\n\n## Jacobian Determinant\n\nJacobian Determinant 是我们在转换坐标系时使用的转换倍率，当坐标系转换时，面积并不一定会固定，而是会拉伸或者压缩。为了考虑这部分的改变，Jacobian Determinant 就是我们改变坐标系所使用的手段，根据 Jacobian matrix 的定义，我们可以知道每个axis 的改变。Determinant of Jacobian matrix 则帮我们找到了其中的变化。\n$$\n\\text{Jacobian Determinant}=\\frac{\\partial(x,y)}{\\partial(u,v)}=\\begin{vmatrix}\n\\frac{\\partial x}{\\partial u}&\\frac{\\partial x}{\\partial v}\\\\\\\\\n\\frac{\\partial y}{\\partial u}&\\frac{\\partial y}{\\partial v}\n\\end{vmatrix}\n$$\n\n### frequently used Jacobian Determinant\n\n#### change to *Cylindrical Coordinates*\n\n$$\nJ(r,\\theta)=\\begin{vmatrix}\\frac{\\partial x}{\\partial r}&\\frac{\\partial x}{\\partial \\theta}\\\\\\\\\n\\frac{\\partial y}{\\partial r}&\\frac{\\partial y}{\\partial \\theta}\n\\end{vmatrix}=\\begin{vmatrix}\\cos\\theta&-r\\sin\\theta\\\\\\\\\n\\sin\\theta&r\\cos\\theta\n\\end{vmatrix}=r\n$$\n\n#### change to *Spherical Coordinates*\n\n\n\n## Smart use of double integral\n\n在 single integral 中，我们计算复杂的面积时往往需要多个integral 相互加减，但是通过将$f(x,y)$ 设定为一个固定的值 1，便可以使用double integral 来计算二维的面积。\n\n## Green's theorem\n\n对于 $\\int_C F\\cdot dr=\\int_CP dx+Qdy$,\n$$\n\\oint_CPdx+Qdy=\\iint_D(\\frac{\\partial Q}{\\partial x}-\\frac{\\partial P}{\\partial y})\\space dA\n$$\n\n> D 是一个闭环区域，且C以逆时针的方式进行\n\n当我们要计算一个闭环所形成的线积分，起点和终点的数值就十分令人疑惑，而 Green's theorem 的诞生让我们的计算更加方便。左边的公式并不难理解，主要需要理解的是右边的公式的诞生。\n\n「图片」\n\n试想象在region D 中，有无数个小的正方块，我们对这个方块做 线积分以求work done，会发现在 $\\vec{F}$ 与行进方向垂直时不会有work done，因此 只有当 $Pdx,Qdy$ 的情况下才有非零积分，且由于平行的两个积分是相反的方向，于是我们就得到了，\n$$\n\\int_{x_1}^{x_2}P()-P()dx+\\int_{y_1}^{y_2}Q()-Q()dy\n$$\n 然后我们反向使用 $\\int_a^b f(x)dx=F(b)-F(a)$，就可以将上面的single integral 转换为double integral。即\n$$\n\\begin{aligned}\n&\\int_{y_1} ^{y_{2}}\\int_{x_1} ^{x_{2}}\\frac{\\partial Q}{\\partial x}dxdy-\\int_{x_1} ^{x_{2}}\\int_{y_1} ^{y_{2}}\\frac{\\partial P}{\\partial x}dydx\\\\\\\\\n=&\\int_{y_1}^{y_2}\\int_{x_1} ^{x_{2}}\\left(\\frac{\\partial Q}{\\partial x}-\\frac{\\partial P}{\\partial y}\\right)dxdy\n\\end{aligned}\n$$\n\n> 当 C 是顺时针是只需要对积分取负即可\n\n# Convert Coordinates system\n\n## Cylindrical Coordinates\n\n$$\n\\begin{aligned}\nx&=r\\cos\\theta\\\\\\\\\ny&=r\\sin\\theta\\\\\\\\\nz&=z\n\\end{aligned}\n$$\n\n## Spherical Coordinates\n\n$$\n\\begin{aligned}\nx&=\\rho\\sin\\phi\\cos \\theta\\\\\\\\\ny&=\\rho\\sin\\phi\\sin\\theta\\\\\\\\\nz&=\\rho\\cos\\phi\n\\end{aligned}\n$$\n\n## ellipse equation\n\n$$\n\\frac{x^2}{a^2}+\\frac{y^2}{b^2}=1\n$$\n\n$$\n\\begin{aligned}\nx&=a\\cos(t)\\\\\ny&=b\\sin(t)\n\\end{aligned}\n$$\n\n\n\n# surface integral\n\n## In vector field\n\n对我来说，计算向量场的面积分其实是在计算向量穿过该面的垂直分量总和，即物理意义上的flux。因此，surface 上的normal vector 的方向尤其重要，因为这会影响我们计算通量的数值。通常而言我们都会取向外的法向量来计算。\n\n### 简单平面\n\n在 $z=g(x,y)$ 这类较为简单的平面中，我们定义 $f(x,y,z)=z-g(x,y)=0$，法向量其实就是梯度($\\nabla f$)  ，因此我们直接使用公式即可\n$$\n\\iint_S\\vec{F}\\cdot d\\vec{S}=\\iint_D\\vec{F}\\cdot\\vec{n}\\space dA\n$$\n\n### 复杂面\n\n当surface 无法简单用一个函数表达时，我们便须要使用 cross product 的特性来找到 法向量。即当$\\vec{r}(u,v)=\\left<x(u,v),y(u,v),z(u,v)\\right>$，我们需要使用$\\vec{r}_u\\times\\vec{r}_v$ 来找法向量，公式为\n$$\n\\iint_D\\vec{F}(u,v)\\cdot (\\vec{r}_u\\times\\vec{r}_v)\\space dudv\n$$\n\n> 注意，所有法向量都要是unit vector，因为我们需要用以计算 F垂直与面的分量\n\n当我们要integral 一个面，但是在 3D 空间中时，我们会将 $x,y,z$ 转换 $u,v$ 来方便使用 double integral，因为通常 triple integral 不能计算 面积\n\n## Divergence Theorem to calculate surface integral\n\n$$\n\\iint_S\\vec{F}\\cdot d\\vec{S}=\\iiint_E\\text{div}\\vec FdV\n$$\n\n## Stroke Theorem\n\n$$\n\\oint_C\\vec{F}\\cdot d\\vec{r}=\\iint_S\\text{curl}\\vec{F}\\cdot d \\vec S\n$$\n\n","categories":["Math"]},{"title":"linear algebra","url":"/2024/04/06/linear-algebra/","content":"\n--------\n\n<!--more-->\n\n# Matrix\n\n## Partitioned Matrixes\n\n将 一个 n*m 的matrix 分成不同的块，例如\n$$\nA_{4\\times4}=\\left[\\begin{array}{ccc|c}\n1&2&3&4\\\\\\\\\n2&3&4&5\\\\\\\\\n\\hline\n3&4&5&6\\\\\\\\\n4&5&6&7\n\\end{array}\\right]=\\begin{bmatrix}\nA_{11}&A_{12}\\\\\\\\\nA_{21}&A_{22}\n\\end{bmatrix}\n$$\n\n### addition\n\n同样规模的划分块可以才直接相加\n\n### multiplication\n\n当 $A$ 的 column 划分 和 $B$ 的 row 划分一致才可以计算$AB$\n\n## <span id=\"2\">augmented matrix(增广矩阵)</span>\n\n通过将 [system of linear equation](#1) 右边的 Co nstant 加入到 左侧的系数矩阵(coefficients matrix) 中，而形成的新矩阵\n$$\n\\begin{bmatrix}\nC|\\text{Constant}\n\\end{bmatrix}=\n\\begin{bmatrix} \\begin{array}{}c_{11}&\\cdots&c_{1n}&\\text{Constant}\\_1\\\\\\\\\n\\vdots&\\ddots&\\vdots&\\vdots\\\\\\\\\nc\\_{m1}&\\cdots&c_{mn}& \\text{Constant}\\_m\n\\end{array}\\end{bmatrix}\n$$\n\n## identity matrix\n\n记作 $I$，只有 $a_{ij}=1$ for $i=j$，其他全零\n$$\nI=\\begin{bmatrix}1&0&0\\\\\\\\0&\\ddots&0\\\\\\\\0&0&1\\end{bmatrix}\n$$\n\n\n## diagonal matrix\n\n事实上 diagonal matrix 只是把空间倍增或者缩小了。例如以下的例子，\n$$\n\\begin{bmatrix}2&0\\\\\\\\0&3\\end{bmatrix}\\begin{bmatrix}1\\\\\\\\1\\end{bmatrix}=\\begin{bmatrix}2\\\\\\\\3\\end{bmatrix}\n$$\n\n\n{% dplayer 'url=linearTransformation_diagonal.mp4' \"autoplay=false\" %} \n\n\n# <span id=\"1\">system of linear equations</span>\n\n## basic form of linear equation\n\n$ax+by+cz+\\cdots=\\text{Constant}$ 是一个linear equation。\n\n> for $x,y,z,\\cdots$ is variables\n\n## pattern of system of linear equations\n\n$$\n\\begin{cases}\nc_{11}x_1+c_{12}x_2+\\cdots+c_{1n}x_n=\\text{Constant}\\_1\\\\\\\\\nc\\_{21}x_1+c\\_{22}x_2+\\cdots+c\\_{2n}x_n=\\text{Constant}\\_2\\\\\\\\\n\t\\cdots\\\\\\\\\nc\\_{m1}x_1+c_{m2}x_2+\\cdots+c_{mn}x_n=\\text{Constant}\\_m\\\\\\\\\n\\end{cases}\n$$\n\n### matrix form\n\n从 row-column rule 我们可以将 system of linear equation 反转成俩个 矩阵乘积\n$$\nC=\\begin{bmatrix}c_{11}&\\cdots&c_{1n}\\\\\\\\\nc_{21}&\\cdots&c_{2n}\\\\\\\\\n\\vdots&\\ddots&\\vdots\\\\\\\\\nc_{m1}&\\cdots&c_{mn}\n\\end{bmatrix},\\quad X=\\begin{bmatrix}\nx_1\\\\\\\\\nx_2\\\\\\\\\n\\vdots\\\\\\\\\nx_n\n\\end{bmatrix},\\quad b=\\begin{bmatrix}\n\\text{Constant}_1\\\\\\\\\n\\text{Constant}_2\\\\\\\\\n\\vdots\\\\\\\\\n\\text{Constant}_m\n\\end{bmatrix},\\quad \\text{For} \\space CX=b\n$$\n\n### vector form\n\n$$\nx_1\\begin{bmatrix}c_{11}\\\\\\\\c_{21}\\\\\\\\\\vdots\\\\\\\\c_{m1}\\end{bmatrix}+x_2\\begin{bmatrix}c_{12}\\\\\\\\c_{22}\\\\\\\\\\vdots\\\\\\\\c_{m2}\\end{bmatrix}+\\cdots+x_n\\begin{bmatrix}c_{1n}\\\\\\\\c_{2n}\\\\\\\\\\vdots\\\\\\\\c_{mn}\\end{bmatrix}=\\begin{bmatrix}\\text{Constant}_1\\\\\\\\\\text{Constant}_2\\\\\\\\\\vdots\\\\\\\\\\text{Constant}_m\\end{bmatrix}\n$$\n\n\n\n### Homogeneous system\n\n$$\nAx=0\n$$\n\n\n\n## Solve the linear system\n\n如果我们将 $Ax=b$ 中的 $A$，看作空间转换的过程，那么我们解linear system 的过程其实就是我们找到$\\vec b$ 在经历空间转换前的vector $\\vec x$，下面的例子中我们便是需要通过 $\\begin{bmatrix}3\\\\\\\\5 \\end{bmatrix}$ 找到$\\begin{bmatrix}2\\\\\\\\3 \\end{bmatrix}$ ，即是其解。\n\n{% dplayer 'url=slove_linear_system.mp4' \"autoplay=false\" %} \n\n我们将 system of linear equations 写成 [augmented matrix](#2) 的形式是为了简化我们计算variables，即matrix $X$，的过程。通过elementary row operation，将augmented matrix 变成 row equivalent 的其他形式。这些 row operation 并不会改变 $CX=b$ 的解。row operation 包括，\n\n- scaling\n- replacement\n- interchange\n\n### Elementary matrix\n\n上面的 row operations 可以被写作一些特殊的初等矩阵(elementary matrix)，例如我们可以将scaling k 倍写作\n$$\nK=\\begin{bmatrix}\nk&0\\\\\\\\\n0&k\n\\end{bmatrix}\n$$\n即 $KCX=Kb$​​ \n\n### Gaussian Elimination\n\n因为 row operation 并不会改变 系统的解，因此我们可以用 [row echelon form](#3) 来找解。 这个过程叫Gaussian Elimination\n\n### Gaussian-Jordan Elimination\n\n$$\nAx=b\n$$\n\n对于 可逆 $A$ ，$x$ 的的解 等于 $bA^{-1}$，因此我们通过 row reduction of $\\begin{bmatrix}A&I\\end{bmatrix}$ to $\\begin{bmatrix}I&A^{-1}\\end{bmatrix}$ to    find out $A^{-1}$ and obtain the solution by $bA^{-1}$.\n\n> refer to [method 2 of finding inverse of matrix](#method2)\n\n### Cramer's Rule\n\n在 Gaussian-Jordan Elimination 中，我们通过row operation 找到 $A^{-1}$，来解 $x$。而Cramer's Rule 则通过使用 坐标系的转换以及 determinant 的特性，直接求 $x,y,z$ 的数值\n$$\nx_i=\\frac{\\det(A(x_i))}{\\det(A)}\n$$\n我们知道determinant 其实在描述坐标系转换时 面积的变化。假设在 x-y 坐标系下 平行四边形的面积为 x，而在一个新坐标系下，平行四边形的面积 Area 是\n$$\n\\begin{aligned}\n\\text{Area}&=x\\det(A)\\\\\\\\\nx&=\\frac{\\text{Area}}{\\det(A)}\\\\\\\\\nx&=\\frac{\\det(A(x))}{\\det(A)}\n\\end{aligned}\n$$\n\n## possibilities of solution of linear system\n\n{%asset_img 1.png%}\n\n### no solution\n\n没有解意味着，有最少两条线是线性有关(linearly dependent)的，只是他们的位置不同。\n\n- $r(A)<r(\\tilde A)$ \n\n### unique solution\n\n也就是说所有线性组合最终会汇集到一个点，而这个点就是解\n\n- $r(A)=r(\\tilde A)=n$\n\n### infinitely many solution\n\n多过一条线重叠，在这些线中有无限相交的点，他们都是一种解, 这种解中我们最少有一个 free variable\n\n- $r(A)=r(\\tilde A)<n$\n\n\n\n# <span id='3'>Row Echelon Form(行阶梯形)</span>\n\n每下一行的第一数字(leading entry) 的位置比上一行要右，且任何行的第一数字下的任何数字都要是零。这种形式的matrix 我们成为 row echelon form。\n\n## reduced echelon form(简化行阶梯形)\n\n当每个 leading entry 所在的那一列，除了leading entry 外其他全是0 时，我们称之为 reduced echelon form。每个leading entry in reduced echelon form 是一个 pivot，而每个有 leading entry in reduced echelon form 的column 是一个pivot column。\n\n# LU factorization(LU 分解)\n\n我们知道解 $Cx=b$ 时我们可以通过 $x=C^{-1}b$ 的方式来找solution。既然$C$ 是一个matrix，我们当然可以将其分解为两个不同的matrix，即$C=LU$，这对我们找$Cx=LUx=b$ 并没有影响，而且通过LU 分解，我们可以然计算更容易因为我们可以选择计算$L(Ux)$ 而不是直接计算$(LU)x$。\n\n## 计算 U matrix\n\n我们可以通过 row operation 的方式将 C 简化为一个 echelon form of matrix，期间我们需要记录每一个elementary matrix，注意:warning:我们不能使用row interchange 因为这回破坏我们的原本的matrix 结构\n\n## 计算 L matrix\n\n因为我们计算 U matrix 的过程实际是通过 $E_k\\cdots E_2E_1C=U$ 得到的。所以根据 $C=LU$ , $E_k\\cdots E_2E_1$ 应为$L^{-1}$, 所以我们只要对$E_k\\cdots E_2E_1$取逆 就可以找到 $L$​了\n$$\n\\begin{aligned}\nE_k\\cdots E_2E_1C&=U\\\\\\\\\nL^{-1}C&=U\\\\\\\\\nL&=(E_k\\cdots E_2E_1)^{-1}\n\\end{aligned}\n$$\n\n\n# Inverse of Matrix\n\n\n\n$A^{-1}$ 是 $A$ 的逆矩阵, where $AA^{-1}=I$. 如果一个矩阵不可逆，那么它就是一个 singular matrix。 相反他就是一个 non-singular matrix。下面的动画展示了 $Ax$ 和 $A^{-1}x$ 的区别\n{% dplayer 'url=inverse_matrix.mp4' \"autoplay=false\" %} \n\n## finding inverse of matrix\n\n### method 1\n\n$$\nA^{-1}=\\frac{1}{\\det(A)}\\text{adj}(A)\n$$\n\nif $\\det(A)\\neq0$，then $A$ is invertible. \n\n## <span id=\"method2\">method 2</span>\n\nBy Gaussian elimination,  we can use the elementary matrix during the transformation to find the inverse of a matrix\n$$\n\\begin{align}{}E_p\\cdots E_1A&=I\\\\\\\\\nA^{-1}&=(E_p\\cdots E_1)\\\\\n\\end{align}\n$$\n对于 矩阵 $[A|I]$，我们将左边乘以 $A^{-1}$ 那么右边自然就会变成$A^{-1}$，即 $[I|A^{-1}]$\n\n### properties\n\nif $A$ is invertible, then $A^T$ is also invertible, because $\\det(A)=\\det(A^T)$\n\n# determinant\n\n直观地说，determinant 计算的是坐标系转换时产生的变化，例如面积的变化等等。\n$$\n\\det (A)=a_{11}C_{11}+a_{12}C_{12}+\\cdots+a_{1n}C_{1n}\n$$\n\nwhere $C$ is cofactor\n$$\nC_{ij}=(-1)^{i+j}\\det(A_{ij})\n$$\n\n## properties\n\n- $\\det(A)=\\det(A^T)$\n- if one row of $A$ is added to another row to produce $B$, then $\\det(A)=\\det(B)$\n- rows interchange **once**, then $\\det(A)=-\\det(B)$\n- $n$ row multiplied by $k$, then $k^n\\det(A)=\\det(B)$\n- $\\det(AB)=\\det(A)\\det(B)$\n- for $A=B^{-1}\\Rightarrow AB=I,\\space\\det(A)\\det(B)=\\det(I)=1\\Rightarrow\\frac{1}{\\det(A)}=\\det(B)$\n\n# Vector\n\n## magnitude (length)\n\n$$\n|\\vec{a}|=\\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}\n$$\n\n- unit vector $\\Rightarrow$ magnitude = 1\n\n## linear combination\n\n$$\nk=a_1\\vec{v}_1+a_2\\vec{v}_2+\\cdots+a_n\\vec{v}_n\n$$\n\nis said to be a linear combination, where $a_1,a_2,\\cdots,a_n$ are scalars and $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_n$ are vectors. \n\n### Linear independence\n\nfor $a_1\\vec{v}_1+a_2\\vec{v}_2+\\cdots+a_n\\vec{v}_n=0$, if exist $a_1,a_2,\\cdots,a_n\\neq0$, then $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_n$ are linearly dependent. 因为，这些向量可以通过不同的scalar来互相消除，而只有有关的vector 才可以互相消除。我们可以将线性有关的vectors看作可以通过倍增或缩减而得到的vector，即它们都有同一个unit vector。\n\n### rank\n\n秩(rank)，记作$r(A)$ 是$A$ 可能的最大数量的线性无关vector。$r(A)$等同于 row echelon form 的非零行。且$r(A)=r(A^T)$。rank 在空间转换中，代表了转换之后的维度。rank 为 1 时，空间转换后便只剩下一条直线。\n\n# Vector Spaces\n\n向量空间是由向量组成的空间，即无论向量如何缩减、倍增、相加、相减，其组合都应该存在与 **向量空间** 之中。平时我们所使用的空间坐标系被称为「欧几里得空间(Euclidean space)」。\n\n一个空集合(empty set) 不能组成一个向量空间，因为不存在不包含任何点的空间，即使是一元向量空间(one-element vector space)，$\\{0\\}$ 也必须包含在其中，因为我们可以用任何向量组成 0 向量。相似的，在$\\mathbb{R}^2$ 中，我们的向量空间也应该包含 $\\{(0,0)\\}$。\n\n## spanning set(生成集)\n\n如果 $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_m$ 是在 $\\mathbb{R}^n$ 中的向量，那么我们会说 Span$\\{\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_m\\}$ 是$\\mathbb{R}^n$ 的子集，即可以想象为在 $\\mathbb{R}^n$ 这个空间中的一部分，这个生成空间由所有$\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_m$​ 的线性组合组成。\n\n## Basis\n\n有时候 生成集(spanning set) 并不是生成 空间的最小集合，例如$\\begin{Bmatrix}\\begin{bmatrix}0\\\\\\\\1\\end{bmatrix},\\begin{bmatrix}1\\\\\\\\0\\end{bmatrix},\\begin{bmatrix}x\\\\\\\\y\\end{bmatrix}\\end{Bmatrix}$ 并不是 $\\mathbb R^2$的最小生成集，其最小生成集是$\\begin{Bmatrix}\\begin{bmatrix}0\\\\\\\\1\\end{bmatrix},\\begin{bmatrix}1\\\\\\\\0\\end{bmatrix}\\end{Bmatrix}$ 那么 $\\begin{bmatrix}x\\\\\\\\y\\end{bmatrix}$ 就显得有点多于，这是因为$\\begin{bmatrix}x\\\\\\\\y\\end{bmatrix}$ 其实是和$\\begin{bmatrix}0\\\\\\\\1\\end{bmatrix},\\begin{bmatrix}1\\\\\\\\0\\end{bmatrix}$​ 线性有关的，因此为了找到这个最小生成集，我们也称为 基 (basis)，我们就需要确保spanning set 中的向量都是线性无关的。\n\n例如我们设 basis 为 $\\begin{Bmatrix}\\begin{bmatrix}2\\\\\\\\3\\end{bmatrix},\\begin{bmatrix}3\\\\\\\\0\\end{bmatrix}\\end{Bmatrix}$\n\n{% dplayer 'url=basis.mp4' \"autoplay=false\" %} \n\n### Some notations\n\n如果我们想用 一个basis $\\mathcal{B}$ 表达一个vector $\\vec{v}$，那我们由两种写法\n$$\nRep_\\mathcal{B}(\\vec{v})=\\begin{bmatrix}c_1\\\\\\\\c_2\\\\\\\\\\vdots\\\\\\\\c_n\\end{bmatrix} \\quad\\text{or}\\quad[\\vec{v}]_\\mathcal{B}=\\begin{bmatrix}c_1\\\\\\\\c_2\\\\\\\\\\vdots\\\\\\\\c_n\\end{bmatrix}\n$$\n\n## dimension\n\n这个和我们平时理解的维度差不多，更具体来讲，dimension 其实是 basis 中的vector 数量，因为 basis 的一个vector 其实就代表了一个轴，因此basis 有几个vector 就代表那个 vector space 有几个轴，也就是其维度(dimension)。\n\n## Orthogonal Basis\n\n从 dot product 的公式，即$\\vec{u}\\cdot\\vec{v}=|\\vec{u}||\\vec{v}|\\cos{\\theta}$ 可以看出来，当dot product 的答案为零时，$\\vec{u}$ 和$\\vec{v}$  就是互相垂直的。\n\n### orthogonal set\n\n当 a set of vectors 中的每一个vector 都垂直于其他vector，那么这个set 便是orthogonal set。而每一个 orthogonal basis for $\\mathbb R^n$都是一个 orthogonal set for $\\mathbb R^n$。\n\n### Gram-Schmidt Orthogonalization Process\n\n我们可以用这个process 来将一个basis 转变成 orthogonal basis。\n$$\n\\vec{u}_k=\\vec{v}\\_k-\\sum^{k-1}\\_{i=1}\\frac{\\left<\\vec{v}_k,\\vec{u}_i\\right>}{|\\vec{u}_i|^2}\\vec{u}_i\n$$\n  更具体的理解这个公式，我们可以将其想象成把第一个vector 定位初始vector，后面的与其垂直的vector ($\\vec{u}_k$)都通过减去在原向量($\\vec{v}_k$​) 在其他所有已得向量的投影，来计算它取其他向量重叠的分量，然后得到与其他向量都不重合的垂直向量。\n\n## Subspace\n\n当一个set $S$ 是 $\\mathbb R^n$ 的子空间时，所有vector in set $S$ 的线性组合都要囊括在该子空间中，即任何子空间的点 都可以通过 set $S$ 求解。\n\n### Column space\n\nColumn space 其实是经过坐标系转换所出现的空间，这个空间中包含了所有 $Ax$ 可能出现的答案。既然column space 是一个空间，那么所有的 pivot column 自然就是这个空间的 basis 了。记作 $Col(A)$。Matrix 的每一个column 其实就代表了 空间转换后的坐标向量。\n\n### Null space (Kernel)\n\n零空间指的是一个空间中所有的向量在经过空间转换后，都会变成 zero vector。即当$Ax=0$​的时候，Null space 就是所有的解形成的空间。记作$Nul(A)$\n\n### properties\n\n$$\ndim\\space Col(A)+dim\\space Nul(A)=n\n$$\n\n$dim$ of $Col(A)$ 是 column space of A 的rank，即使basis 的dimension。对于上面的特性我们可以这样想，如果rank 和 A 的column 数目一样，那么空间转换之后其实空间的dimension 并没有变过，因此无法将高维的向量压缩到 0 点。但是当rank of A $\\neq$ column 的数目的时候，代表空间会被压缩，剩下多少维度，Null space 的维度就可以是多大。\n\n# Transforamtion\n\n我们可以将 $Ax=b$ 中的 $A$ 看作一个function，将 $x$ 变成 $b$，通常叫做transformation。当我们将一个 $\\mathbb R^n$ 中的向量 $x$ 转变成 $\\mathbb R^m$ 中的向量 $T(x)$ 时，我们会称$\\mathbb R^n$ 为domain of T，$\\mathbb R^m$ 为codomain of T。\n$$\n\\begin{aligned}\nT:\\mathbb R^n &\\rightarrow \\mathbb R^m\\quad\\text{OR}\\\\\\\\\nx&\\mapsto Ax\n\\end{aligned}\n$$\n\n## onto\n\n当每一个点 $x$，在linear transformation 之后都对应了最少一个点 $b$ in $\\mathbb R^m$，那么$T$ 就是 onto $\\mathbb R^m$\n$$\nT(x)=b\n$$\n\n## one-to-one\n\n当每一个点 $x$，在linear transformation 之后都对应了最多一个点 $b$ in $\\mathbb R^m$，那么$T$ 就是 one-to-one 的。即 $\\mathbb R^n$ 的点不可以收敛到 $\\mathbb R^m$，但是可扩散到 $\\mathbb R^m$​。\n\n## Projection Transformation\n\n把所有的点压平到同一平面上，叫做投影。例如，将在三维空间中的点，全部垂直放到x,y 平面上就叫projection transformation\n\n## Shear Transformation\n\n当 $x$ 轴维持不动，但是 $y$ 轴的 vector 从(0,1) 变成 (1,1)，也就是将正方形变成平行四边形的过程叫做 shear transformation。\n\n# Eigenspace\n\n## Eigenvector\n\n这个向量指的是在经过转换之后依然保持在同一位置，只不过是被增大或缩小的向量，即\n$$\nA\\vec{v}=\\lambda\\vec{v}\n$$\n可以将其理解为经过transoformation 后 其spanning space 并不会改变。而 $\\lambda$​ 我们就成为 eigenvalue。 \n\n## Eigenvalue\n\n既然 Eigenvalue 是一个 scalar，那么我们就可以将其写成 $\\lambda I$ 的形式的diagonal matrix，也就是，\n$$\n\\begin{bmatrix}\n\\lambda&0&0\\\\\\\\\n0&\\lambda&0\\\\\\\\\n0&0&\\lambda\n\\end{bmatrix}\n$$\n\n即Eigenvalue 被倍增的值\n\n### finding eigenvector $\\vec{v}$ and eigenvalue $\\lambda$\n\n通过公式我们可得，\n$$\n\\begin{aligned}\nA\\vec{v}-\\lambda\\vec{v}&=0\\\\\\\\\n(A-\\lambda I)\\vec{v}&=0\n\\end{aligned}\n$$\n在我们得到的公式中其实 $\\vec{v}$ 是一个 Null space。而根据我们对determinant 的理解(~~可能没写出来~~)，只有当determinant = 0 的时候才可以将向量的维度压低。因此我们可以通过 $\\det(A-\\lambda I)=0$，来找到 $\\lambda$，又或者通过solving linear system 的方法找到 $\\vec{v}$​。\n\n## Egienspace\n\nEgienspace 即span by eigenvector 的空间，不同的eigenvalue 有不同的spanning set\n\n# Similar Matrix\n\n在我的理解中，similar matrix 指的是，matrix $A$ 经过transformation 后再逆transformation 后的matrix $B$ 就是 matrix $A$ 的similar matrix。而这两个matrix 的charateristic equation是一样的，也就是说这两个matrix 对eigenvector的影响是一样的。\n$$\nA=PDP^{-1}\n$$\nA similar to D\n\n## diagonalizable\n\n我们称 $A$ diagonalizable，当$n\\times n$ matrix $A$ 有 $n$ 个线性无关的eigenvectors，$D$ 是一个diagonal matrix。简单来说就是 $A$ 这个空间转换的matrix 可以被 通过 $P$ 和 $P^{-1}$​ 中间被乘以一个diagonal matrix 来代替。而这个diagonal matrix 则应该是对于n个不同的eigenvector 的倍率，即eigenvalue，其次序与P 中eigenvector 的次序有关。本质上 diagonalizable 是把不同eigenvalue 的eigenvector 整合起来，一同计算，或者说找到A 对于所有eigenvector 的影响。\n\n{% dplayer 'url=diagonalizable_matrix.mp4' \"autoplay=false\" %}\n\n事实上只有P 的column 是n个线性无关的时候 A 才是diagonalizable的，我这样理解\n$$\n\\begin{aligned}\nA&=PDP^{-1}\\\\\\\\\nAP&=PD\\\\\\\\\nA\\begin{bmatrix}\\vec{v}_1&\\vec{v}_2&\\cdots&\\vec{v}_n\\end{bmatrix}&=\\begin{bmatrix}\\lambda_1\\vec{v}_1&\\lambda_2\\vec{v}_2&\\cdots&\\lambda_n\\vec{v}_n\\end{bmatrix}\n\\end{aligned}\n$$\n根据 eigenvector 的formula $A\\vec{v}=\\lambda\\vec{v}$ 如果 $\\vec{v}_1,\\cdots,\\vec{v}_n$ 不全为eigenvector 的话，就会令公式 $A=PDP^{-1}$不成立。\n\n# Orthonormal Set\n\n如果所有vectors in orthogonal set are unit vector，那么这个set 就是 orthonormal set。\n\n如果 vectors $\\vec v$ 垂直于W 空间或其中所有vector，那么$\\vec v$ 就是orthogonal complement of W，记作 $W^{\\bot}$\n\n当 $A$ 有 orthonormal columns，那么$A^TA=I$，也就是说只有当自己 dot 自己才是1，而自己 dot 其他的，则是0，\n$$\nA^TA=\\begin{bmatrix}a_1^T\\\\\\\\a_2^T\\end{bmatrix}\\begin{bmatrix}a_1&a_2\\end{bmatrix}=\\begin{bmatrix}a_1^Ta_1&a_1^Ta_2\\\\\\\\a_2^Ta_1&a_2^Ta_2\\end{bmatrix}=\\begin{bmatrix}1&0\\\\\\\\0&1\\end{bmatrix}\n$$\n ## orthonormal matrix\n\northonormal matrix 是一个 $n\\times m$ matrix，not need to be square。且其所有column 都是orthonormal columns。\n\n## orthogonal matrix\n\northogonal matrix = Square matrix + orthonormal column，即 orthongonal matrix 是 一个 $n\\times n$ matrix 同时 $A^T=A^{-1}$，且 all column are orthonormal\n\n\n\n## Orthogonally diagonalizable\n\n我们把 orthonormal 和 diagonalizable 结合，我们得到\n$$\nA=PDP^{-1}=PDP^T\n$$\n","categories":["Math"]}]