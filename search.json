[{"title":"linear algebra","url":"/2024/04/06/linear-algebra/","content":"\n# Matrix\n\n<!--more-->\n\n## Partitioned Matrixes\n\n将 一个 n*m 的matrix 分成不同的块，例如\n$$\nA_{4\\times4}=\\left[\\begin{array}{ccc|c}\n1&2&3&4\\\\\\\\\n2&3&4&5\\\\\\\\\n\\hline\n3&4&5&6\\\\\\\\\n4&5&6&7\n\\end{array}\\right]=\\begin{bmatrix}\nA_{11}&A_{12}\\\\\\\\\nA_{21}&A_{22}\n\\end{bmatrix}\n$$\n\n### addition\n\n同样规模的划分块可以才直接相加\n\n### multiplication\n\n当 $A$ 的 column 划分 和 $B$ 的 row 划分一致才可以计算$AB$\n\n## <span id=\"2\">augmented matrix(增广矩阵)</span>\n\n通过将 [system of linear equation](#1) 右边的 Constant 加入到 左侧的系数矩阵(coefficients matrix) 中，而形成的新矩阵\n$$\n\\begin{bmatrix}\nC|\\text{Constant}\n\\end{bmatrix}=\n\\begin{bmatrix} \\begin{array}{}c_{11}&\\cdots&c_{1n}&\\text{Constant}\\_1\\\\\\\\\n\\vdots&\\ddots&\\vdots&\\vdots\\\\\\\\\nc\\_{m1}&\\cdots&c_{mn}& \\text{Constant}\\_m\n\\end{array}\\end{bmatrix}\n$$\n\n## identity matrix\n\n\n\n# <span id=\"1\">system of linear equations</span>\n\n## basic form of linear equation\n\n$ax+by+cz+\\cdots=\\text{Constant}$ 是一个linear equation。\n\n> for $x,y,z,\\cdots$ is variables\n\n## pattern of system of linear equations\n\n$$\n\\begin{cases}\nc_{11}x_1+c_{12}x_2+\\cdots+c_{1n}x_n=\\text{Constant}\\_1\\\\\\\\\nc\\_{21}x_1+c\\_{22}x_2+\\cdots+c\\_{2n}x_n=\\text{Constant}\\_2\\\\\\\\\n\t\\cdots\\\\\\\\\nc\\_{m1}x_1+c_{m2}x_2+\\cdots+c_{mn}x_n=\\text{Constant}\\_m\\\\\\\\\n\\end{cases}\n$$\n\n### matrix form\n\n从 row-column rule 我们可以将 system of linear equation 反转成俩个 矩阵乘积\n$$\nC=\\begin{bmatrix}c_{11}&\\cdots&c_{1n}\\\\\\\\\nc_{21}&\\cdots&c_{2n}\\\\\\\\\n\\vdots&\\ddots&\\vdots\\\\\\\\\nc_{m1}&\\cdots&c_{mn}\n\\end{bmatrix},\\quad X=\\begin{bmatrix}\nx_1\\\\\\\\\nx_2\\\\\\\\\n\\vdots\\\\\\\\\nx_n\n\\end{bmatrix},\\quad b=\\begin{bmatrix}\n\\text{Constant}_1\\\\\\\\\n\\text{Constant}_2\\\\\\\\\n\\vdots\\\\\\\\\n\\text{Constant}_m\n\\end{bmatrix},\\quad \\text{For} \\space CX=b\n$$\n\n### vector form\n\n$$\nx_1\\begin{bmatrix}c_{11}\\\\\\\\c_{21}\\\\\\\\\\vdots\\\\\\\\c_{m1}\\end{bmatrix}+x_2\\begin{bmatrix}c_{12}\\\\\\\\c_{22}\\\\\\\\\\vdots\\\\\\\\c_{m2}\\end{bmatrix}+\\cdots+x_n\\begin{bmatrix}c_{1n}\\\\\\\\c_{2n}\\\\\\\\\\vdots\\\\\\\\c_{mn}\\end{bmatrix}=\\begin{bmatrix}\\text{Constant}_1\\\\\\\\\\text{Constant}_2\\\\\\\\\\vdots\\\\\\\\\\text{Constant}_m\\end{bmatrix}\n$$\n\n\n\n### Homogeneous system\n\n$$\nAx=0\n$$\n\n\n\n## Solve the linear system\n\n我们将 system of linear equations 写成 [augmented matrix](#2) 的形式是为了简化我们计算variables，即matrix $X$，的过程。通过elementary row operation，将augmented matrix 变成 row equivalent 的其他形式。这些 row operation 并不会改变 $CX=b$ 的解。row operation 包括，\n\n- scaling\n- replacement\n- interchange\n\n### Elementary matrix\n\n上面的 row operations 可以被写作一些特殊的初等矩阵(elementary matrix)，例如我们可以将scaling k 倍写作\n$$\nK=\\begin{bmatrix}\nk&0\\\\\\\\\n0&k\n\\end{bmatrix}\n$$\n即 $KCX=Kb$​​ \n\n### Gaussian Elimination\n\n因为 row operation 并不会改变 系统的解，因此我们可以用 [row echelon form](#3) 来找解。 这个过程叫Gaussian Elimination\n\n### Gaussian-Jordan Elimination\n\n$$\nAx=b\n$$\n\n对于 可逆 $A$ ，$x$ 的的解 等于 $bA^{-1}$，因此我们通过 row reduction of $\\begin{bmatrix}A&I\\end{bmatrix}$ to $\\begin{bmatrix}I&A^{-1}\\end{bmatrix}$ to    find out $A^{-1}$ and obtain the solution by $bA^{-1}$.\n\n> refer to [method 2 of finding inverse of matrix](#method2)\n\n### Cramer's Rule\n\n在 Gaussian-Jordan Elimination 中，我们通过row operation 找到 $A^{-1}$，来解 $x$。而Cramer's Rule 则通过使用 坐标系的转换以及 determinant 的特性，直接求 $x,y,z$ 的数值\n$$\nx_i=\\frac{\\det(A(x_i))}{\\det(A)}\n$$\n我们知道determinant 其实在描述坐标系转换时 面积的变化。假设在 x-y 坐标系下 平行四边形的面积为 x，而在一个新坐标系下，平行四边形的面积 Area 是\n$$\n\\begin{aligned}\n\\text{Area}&=x\\det(A)\\\\\\\\\nx&=\\frac{\\text{Area}}{\\det(A)}\\\\\\\\\nx&=\\frac{\\det(A(x))}{\\det(A)}\n\\end{aligned}\n$$\n\n## possibilities of solution of linear system\n\n{%asset_img 1.png%}\n\n### no solution\n\n没有解意味着，有最少两条线是线性有关(linearly dependent)的，只是他们的位置不同。\n\n- $r(A)<r(\\tilde A)$ \n\n### unique solution\n\n也就是说所有线性组合最终会汇集到一个点，而这个点就是解\n\n- $r(A)=r(\\tilde A)=n$\n\n### infinitely many solution\n\n多过一条线重叠，在这些线中有无限相交的点，他们都是一种解, 这种解中我们最少有一个 free variable\n\n- $r(A)=r(\\tilde A)<n$\n\n\n\n# <span id='3'>Row Echelon Form(行阶梯形)</span>\n\n每下一行的第一数字(leading entry) 的位置比上一行要右，且任何行的第一数字下的任何数字都要是零。这种形式的matrix 我们成为 row echelon form。\n\n## reduced echelon form(简化行阶梯形)\n\n当每个 leading entry 所在的那一列，除了leading entry 外其他全是0 时，我们称之为 reduced echelon form。每个leading entry in reduced echelon form 是一个 pivot，而每个有 leading entry in reduced echelon form 的column 是一个pivot column。\n\n# LU factorization(LU 分解)\n\n我们知道解 $Cx=b$ 时我们可以通过 $x=C^{-1}b$ 的方式来找solution。既然$C$ 是一个matrix，我们当然可以将其分解为两个不同的matrix，即$C=LU$，这对我们找$Cx=LUx=b$ 并没有影响，而且通过LU 分解，我们可以然计算更容易因为我们可以选择计算$L(Ux)$ 而不是直接计算$(LU)x$。\n\n## 计算 U matrix\n\n我们可以通过 row operation 的方式将 C 简化为一个 echelon form of matrix，期间我们需要记录每一个elementary matrix，注意:warning:我们不能使用row interchange 因为这回破坏我们的原本的matrix 结构\n\n## 计算 L matrix\n\n因为我们计算 U matrix 的过程实际是通过 $E_k\\cdots E_2E_1C=U$ 得到的。所以根据 $C=LU$ , $E_k\\cdots E_2E_1$ 应为$L^{-1}$, 所以我们只要对$E_k\\cdots E_2E_1$取逆 就可以找到 $L$了\n\n# Inverse of Matrix\n\n\n\n$A^{-1}$ 是 $A$ 的逆矩阵, where $AA^{-1}=I$. 如果一个矩阵不可逆，那么它就是一个 singular matrix。 相反他就是一个 non-singular matrix。\n\n## finding inverse of matrix\n\n### method 1\n\n$$\nA^{-1}=\\frac{1}{\\det(A)}\\text{adj}(A)\n$$\n\nif $\\det(A)\\neq0$，then $A$ is invertible. \n\n## <span id=\"method2\">method 2</span>\n\nBy Gaussian elimination,  we can use the elementary matrix during the transformation to find the inverse of a matrix\n$$\n\\begin{align}{}E_p\\cdots E_1A&=I\\\\\\\\\nA^{-1}&=(E_p\\cdots E_1)\\\\\n\\end{align}\n$$\n对于 矩阵 $[A|I]$，我们将左边乘以 $A^{-1}$ 那么右边自然就会变成$A^{-1}$，即 $[I|A^{-1}]$\n\n### properties\n\nif $A$ is invertible, then $A^T$ is also invertible, because $\\det(A)=\\det(A^T)$\n\n# determinant\n\n直观地说，determinant 计算的是坐标系转换时产生的变化，例如面积的变化等等。\n$$\n\\det (A)=a_{11}C_{11}+a_{12}C_{12}+\\cdots+a_{1n}C_{1n}\n$$\n\nwhere $C$ is cofactor\n$$\nC_{ij}=(-1)^{i+j}\\det(A_{ij})\n$$\n\n## properties\n\n- $\\det(A)=\\det(A^T)$\n- if one row of $A$ is added to another row to produce $B$, then $\\det(A)=\\det(B)$\n- rows interchange **once**, then $\\det(A)=-\\det(B)$\n- $n$ row multiplied by $k$, then $k^n\\det(A)=\\det(B)$\n- $\\det(AB)=\\det(A)\\det(B)$\n- for $A=B^{-1}\\Rightarrow AB=I,\\space\\det(A)\\det(B)=\\det(I)=1\\Rightarrow\\frac{1}{\\det(A)}=\\det(B)$\n\n# Vector\n\n## magnitude (length)\n\n$$\n|\\vec{a}|=\\sqrt{a_1^2+a_2^2+\\cdots+a_n^2}\n$$\n\n- unit vector $\\Rightarrow$ magnitude = 1\n\n## linear combination\n\n$$\nk=a_1\\vec{v}_1+a_2\\vec{v}_2+\\cdots+a_n\\vec{v}_n\n$$\n\nis said to be a linear combination, where $a_1,a_2,\\cdots,a_n$ are scalars and $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_n$ are vectors. \n\n### Linear independence\n\nfor $a_1\\vec{v}_1+a_2\\vec{v}_2+\\cdots+a_n\\vec{v}_n=0$, if exist $a_1,a_2,\\cdots,a_n\\neq0$, then $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_n$ are linearly dependent. 因为，这些向量可以通过不同的scalar来互相消除，而只有有关的vector 才可以互相消除。我们可以将线性有关的vectors看作可以通过倍增或缩减而得到的vector，即它们都有同一个unit vector。\n\n### rank\n\n秩(rank)，记作$r(A)$ 是$A$ 可能的最大数量的线性无关vector。$r(A)$等同于 row echelon form 的非零行。且$r(A)=r(A^T)$\n\n# Vector Spaces\n\n向量空间是由向量组成的空间，即无论向量如何缩减、倍增、相加、相减，其组合都应该存在与 **向量空间** 之中。平时我们所使用的空间坐标系被称为「欧几里得空间(Euclidean space)」。\n\n一个空集合(empty set) 不能组成一个向量空间，因为不存在不包含任何点的空间，即使是一元向量空间(one-element vector space)，$\\{0\\}$ 也必须包含在其中，因为我们可以用任何向量组成 0 向量。相似的，在$\\mathbb{R}^2$ 中，我们的向量空间也应该包含 $\\{(0,0)\\}$。\n\n## spanning set(生成集)\n\n如果 $\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_m$ 是在 $\\mathbb{R}^n$ 中的向量，那么我们会说 Span$\\{\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_m\\}$ 是$\\mathbb{R}^n$ 的子集，即可以想象为在 $\\mathbb{R}^n$ 这个空间中的一部分，这个生成空间由所有$\\vec{v}_1,\\vec{v}_2,\\cdots,\\vec{v}_m$​ 的线性组合组成。\n\n## Basis\n\n有时候 生成集(spanning set) 并不是生成 空间的最小集合，例如$\\begin{Bmatrix}\\begin{bmatrix}0\\\\\\\\1\\end{bmatrix},\\begin{bmatrix}1\\\\\\\\0\\end{bmatrix},\\begin{bmatrix}x\\\\\\\\y\\end{bmatrix}\\end{Bmatrix}$ 并不是 $\\mathbb R^2$的最小生成集，其最小生成集是$\\begin{Bmatrix}\\begin{bmatrix}0\\\\\\\\1\\end{bmatrix},\\begin{bmatrix}1\\\\\\\\0\\end{bmatrix}\\end{Bmatrix}$ 那么 $\\begin{bmatrix}x\\\\\\\\y\\end{bmatrix}$ 就显得有点多于，这是因为$\\begin{bmatrix}x\\\\\\\\y\\end{bmatrix}$ 其实是和$\\begin{bmatrix}0\\\\\\\\1\\end{bmatrix},\\begin{bmatrix}1\\\\\\\\0\\end{bmatrix}$ 线性有关的，因此为了找到这个最小生成集，我们也称为 基 (basis)，我们就需要确保spanning set 中的向量都是线性无关的。\n\n### Some notations\n\n如果我们想用 一个basis $\\mathcal{B}$ 表达一个vector $\\vec{v}$，那我们由两种写法\n$$\nRep_\\mathcal{B}(\\vec{v})=\\begin{bmatrix}c_1\\\\\\\\c_2\\\\\\\\\\vdots\\\\\\\\c_n\\end{bmatrix} \\quad\\text{or}\\quad[\\vec{v}]_\\mathcal{B}=\\begin{bmatrix}c_1\\\\\\\\c_2\\\\\\\\\\vdots\\\\\\\\c_n\\end{bmatrix}\n$$\n\n## dimension\n\n这个和我们平时理解的维度差不多，更具体来讲，dimension 其实是 basis 中的vector 数量，因为 basis 的一个vector 其实就代表了一个轴，因此basis 有几个vector 就代表那个 vector space 有几个轴，也就是其维度(dimension)。\n\n## Orthogonal Basis\n\n从 dot product 的公式，即$\\vec{u}\\cdot\\vec{v}=|\\vec{u}||\\vec{v}|\\cos{\\theta}$ 可以看出来，当dot product 的答案为零时，$\\vec{u}$ 和$\\vec{v}$  就是互相垂直的。\n\n### orthogonal set\n\n当 a set of vectors 中的每一个vector 都垂直于其他vector，那么这个set 便是orthogonal set。而每一个 orthogonal basis for $\\mathbb R^n$都是一个 orthogonal set for $\\mathbb R^n$。\n\n### Gram-Schmidt Orthogonalization Process\n\n我们可以用这个process 来将一个basis 转变成 orthogonal basis。\n$$\n\\vec{u}_k=\\vec{v}\\_k-\\sum^{k-1}\\_{i=1}\\frac{\\left<\\vec{v}_k,\\vec{u}_i\\right>}{|\\vec{u}_i|^2}\\vec{u}_i\n$$\n  更具体的理解这个公式，我们可以将其想象成把第一个vector 定位初始vector，后面的与其垂直的vector ($\\vec{u}_k$)都通过减去在原向量($\\vec{v}_k$) 在其他所有已得向量的投影，来计算它取其他向量重叠的分量，然后得到与其他向量都不重合的垂直向量。\n","categories":["Math"]}]